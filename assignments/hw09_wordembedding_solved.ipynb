{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding\n",
    "**In case of any questions email us: deeplearninginsciences@gmail.com**  \n",
    "**Author: Bálint Ármin Pataki**\n",
    "\n",
    "In this notebook we will create a model that can find a meaningful vector representation of words based on a large text file (corpus). This is an unsupervised learning task, as we do not have any labels (like we had for the happy-sad pictures). We have only the raw text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load numpy\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **toy corpus** (copied from https://en.wikipedia.org/wiki/Text_corpus) we will use is the following:\n",
    "\n",
    "_A corpus may contain texts in a single language (monolingual corpus) or text data in multiple languages (multilingual corpus).\n",
    "Multilingual corpora that have been specially formatted for side-by-side comparison are called aligned parallel corpora. There are two main types of parallel corpora which contain texts in two languages. In a translation corpus, the texts in one language are translations of texts in the other language. In a comparable corpus, the texts are of the same kind and cover the same content, but they are not translations of each other. To exploit a parallel text, some kind of text alignment identifying equivalent text segments (phrases or sentences) is a prerequisite for analysis. Machine translation algorithms for translating between two languages are often trained using parallel fragments comprising a first language corpus and a second language corpus which is an element-for-element translation of the first language corpus.\n",
    "In order to make the corpora more useful for doing linguistic research, they are often subjected to a process known as annotation. An example of annotating a corpus is part-of-speech tagging, or POS-tagging, in which information about each word's part of speech (verb, noun, adjective, etc.) is added to the corpus in the form of tags. Another example is indicating the lemma (base) form of each word. When the language of the corpus is not a working language of the researchers who use it, interlinear glossing is used to make the annotation bilingual._\n",
    "\n",
    "Our **example sentence** will be: _The quick brown fox jumps over the lazy dog_\n",
    "\n",
    "#### Let's save it to a python variable as a string!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'A corpus may contain texts in a single language (monolingual corpus) or text data in multiple languages (multilingual corpus). Multilingual corpora that have been specially formatted for side-by-side comparison are called aligned parallel corpora. There are two main types of parallel corpora which contain texts in two languages. In a translation corpus, the texts in one language are translations of texts in the other language. In a comparable corpus, the texts are of the same kind and cover the same content, but they are not translations of each other. To exploit a parallel text, some kind of text alignment identifying equivalent text segments (phrases or sentences) is a prerequisite for analysis. Machine translation algorithms for translating between two languages are often trained using parallel fragments comprising a first language corpus and a second language corpus which is an element-for-element translation of the first language corpus. In order to make the corpora more useful for doing linguistic research, they are often subjected to a process known as annotation. An example of annotating a corpus is part-of-speech tagging, or POS-tagging, in which information about each word\\'s part of speech (verb, noun, adjective, etc.) is added to the corpus in the form of tags. Another example is indicating the lemma (base) form of each word. When the language of the corpus is not a working language of the researchers who use it, interlinear glossing is used to make the annotation bilingual.'\n",
    "example_sentece = 'The quick brown fox jumps over the lazy dog'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process when the input text (corpus) is splitted into smaller parts, called tokens. Token and word have often similar meaning, but not necessarily.\n",
    "\n",
    "\n",
    "#### For example (the text in the parenthesis is one token):   \n",
    " - _The quick brown fox jumps over the lazy dog._  \n",
    " \n",
    "will be converted to:  \n",
    " - (The), (quick), (brown), (fox), (jumps), (over), (the), (lazy), (dog)\n",
    "\n",
    "**But is some case it is not completely clear:**  \n",
    " - _It's not clear._ \n",
    " \n",
    "can be:\n",
    " - (It's), (not), (clear)\n",
    " - (It), (s), (not), (clear)\n",
    " - (It), ('s), (not), (clear)\n",
    "\n",
    "\n",
    "We will implement a tokenization when we split text an all non alphanumeric characters!  \n",
    "So we pick:\n",
    " - (It), (s), (not), (clear)\n",
    " \n",
    "## Let's implement a tokenizer!\n",
    "### Help:\n",
    "#### get_non_alphanumeric_chars\n",
    " 1. with the `list()` function it is easy to convert the input to list of characters\n",
    " 2. with the `.isalnum()` function you can filter out all the non alphanumeric characters\n",
    " 3. `list(set(duplicated_list))` will keep only unique values in the list\n",
    " \n",
    " \n",
    "#### tokenizer \n",
    " 1. replace all non alphanumeric characters to whitespace! Use `.replace()` function.\n",
    " 2. do the splitting via the `.split()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def get_non_alphanumeric_chars(input_text):\n",
    "    \"\"\"\n",
    "        Returns the non alphanumeric unique characters from a corpus.\n",
    "        Input: \n",
    "            * input_text:   string\n",
    "        Output:\n",
    "            * non_alphanumeric_unique: list of non alphanumeric characters\n",
    "    \"\"\"     \n",
    "    ###Start code here\n",
    "    \n",
    "    #STEP1 set character_list variable to a list of characters in the input_text \n",
    "    character_list = list(input_text)\n",
    "    \n",
    "    #STEP2 store all non_alphanumeric characters from the character_list!\n",
    "    non_alphanumeric = [c for c in character_list if not(c.isalnum())]\n",
    "    \n",
    "    #STEP3 keep only the unique characters (hint: unique_values = list(set(duplicated_values))\n",
    "    non_alphanumeric_unique = list(set(non_alphanumeric))\n",
    "    \n",
    "    ###End code here \n",
    "    \n",
    "    return non_alphanumeric_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ',', '(', ')', '-', '.', \"'\"]\n",
      "[' ', '(', '$', '/', ')', '=']\n"
     ]
    }
   ],
   "source": [
    "print(get_non_alphanumeric_chars(corpus))\n",
    "print(get_non_alphanumeric_chars('bla$$=()/ma z'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output** (maybe you will have different order but that is fine):\n",
    "\n",
    "<pre>['.', ',', '-', \"'\", '(', ' ', ')']  \n",
    "['/', '=', ' ', '(', '$', ')']\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def tokenizer(input_text):\n",
    "    \"\"\"\n",
    "        Transforms strings to tokens. The tokens should appear in the same order as they are in the string.\n",
    "        Input: \n",
    "            * input_text:   string\n",
    "        Output:\n",
    "            * tokens: list of tokens\n",
    "    \"\"\"    \n",
    "\n",
    "    \n",
    "    ###Start code here\n",
    "    #STEP0 extract non alphanumeric characters! Use your function above!\n",
    "    non_alphanumeric_unique = get_non_alphanumeric_chars(input_text)\n",
    "    \n",
    "    \n",
    "    #STEP1 replace all non alphanumeric characters to ' ' whitespace.\n",
    "    replaced_input =input_text\n",
    "    for c in non_alphanumeric_unique:\n",
    "        replaced_input = replaced_input.replace(c, ' ')\n",
    "    \n",
    "    #STEP2 split the input_text on all the non_alphanumeric_unique characters and store then in the tokens variable\n",
    "    tokens = replaced_input.split()\n",
    "    \n",
    "    ###End code here    \n",
    "    \n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['monolingual', 'corpus', 'or', 'text', 'data', 'in', 'multiple', 'languages', 'multilingual', 'corpus', 'Multilingual', 'corpora', 'that']\n",
      "\n",
      "['Hi', 'How', 'are', 'you', 'I', 'm', 'fine', 'thanks', 'Bye']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(corpus[49:153]))\n",
    "print()\n",
    "print(tokenizer('Hi! How are you? I\\'m fine, thanks... Bye'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "<pre>\n",
    "['monolingual', 'corpus', 'or', 'text', 'data', 'in', 'multiple', 'languages', 'multilingual', 'corpus', 'Multilingual', 'corpora', 'that']<br>\n",
    "['Hi', 'How', 'are', 'you', 'I', 'm', 'fine', 'thanks', 'Bye']\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stemming\n",
    "This is the process when the inflected words are converted to their root word.\n",
    "\n",
    "For example: goes $\\to$ go, stemming $\\to$ stem, fishing $\\to$ fish\n",
    "\n",
    "This can help a lot when the corpus is not large enough because then we do not have that many occurence of a word. And having just an _s_ at the end of the word means a completely different word as we will see below.\n",
    "\n",
    "\n",
    "It is not that easy to write a proper stemmer, so we will **skip** this part in the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dictionary & one-hot encoding\n",
    "\n",
    "When the tokenization is ready we can build up a dictionary of the tokens. This is simply the sorted version of the tokens.\n",
    "\n",
    "A dictionary looks like: ['a', 'an', 'apple', ..., 'orange', ..., 'zebra', 'zulu']\n",
    "\n",
    "From now we can think of a word as a vector which has 1 at the position of the word in the dictionary, and 0-s anywhere else.\n",
    "\n",
    "$a = \\begin{bmatrix}\n",
    "           1 \\\\\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "           0\n",
    "         \\end{bmatrix},\\:\\:$ \n",
    "$an = \\begin{bmatrix}\n",
    "           0 \\\\\n",
    "           1 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "           0\n",
    "         \\end{bmatrix},\\:\\:$\n",
    "$apple = \\begin{bmatrix}\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           1 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "           0\n",
    "         \\end{bmatrix}\\:\\: ... \\:\\:$\n",
    "$zebra = \\begin{bmatrix}\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           1 \\\\\n",
    "           0\n",
    "         \\end{bmatrix},\\:\\:$\n",
    "$zulu = \\begin{bmatrix}\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "           1\n",
    "         \\end{bmatrix}$\n",
    "         \n",
    "         \n",
    "### Let's build a dictionary and the one-hot encoding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def get_dictionary(input_text):\n",
    "    \"\"\"\n",
    "        Builds up the dictionary from an input text.\n",
    "        Input: \n",
    "            * input_text:   string\n",
    "        Output:\n",
    "            * dictionary: list of unique, sorted tokens\n",
    "    \"\"\"    \n",
    "    tokens = tokenizer(input_text)\n",
    "    ###Start code here\n",
    "    \n",
    "    unique_tokens = list(set(tokens))\n",
    "    unique_sorted_tokens = sorted(unique_tokens)\n",
    "    \n",
    "    ###End code here\n",
    "    dictionary = unique_sorted_tokens\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'added', 'adjective', 'algorithms', 'aligned', 'alignment', 'an', 'analysis', 'and']\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "print(get_dictionary(corpus.lower())[0:10])\n",
    "print(len(get_dictionary(corpus.lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "<pre>\n",
    "['a', 'about', 'added', 'adjective', 'algorithms', 'aligned', 'alignment', 'an', 'analysis', 'and']  \n",
    "118\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def one_hot_encoded(word, dictionary):\n",
    "    \"\"\"\n",
    "        Turn a word to a one-hot encoded vector based on the dictionary.\n",
    "        Input: \n",
    "            * word:   string\n",
    "            * dictionary: list of words in a dictionary\n",
    "        Output:\n",
    "            * oh_vec: one-hot encoded vector\n",
    "    \"\"\"    \n",
    "    #check if word is in dictionary\n",
    "    if(not word in dictionary):\n",
    "        print('The word _' + word + '_ is not in dictionary!')\n",
    "        return None\n",
    "    \n",
    "    ###Start code here\n",
    "    # oh_vec should be a numpy array with a shape (len(dictionary), 1)\n",
    "    # it should contains 0 everywhere except for the position of the word in the dictionary. There is should be 1. \n",
    "    oh_vec = np.zeros((len(dictionary),1), dtype='int')\n",
    "    oh_vec[dictionary.index(word)] = 1\n",
    "    \n",
    "    ###End code here\n",
    "\n",
    "    return oh_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "The word _ablak_ is not in dictionary!\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dic = get_dictionary(corpus.lower())\n",
    "\n",
    "print(one_hot_encoded('a', dic)[0:10])\n",
    "print()\n",
    "print(one_hot_encoded('aligned', dic)[0:10])\n",
    "print()\n",
    "print(one_hot_encoded('ablak', dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "<pre>\n",
    "[[1]\n",
    " [0]\n",
    " [0]\n",
    " [0]\n",
    " [0]\n",
    " [0]\n",
    " [0]\n",
    " [0]\n",
    " [0]\n",
    " [0]]\n",
    "\n",
    "[[0]\n",
    " [0]\n",
    " [0]\n",
    " [0]\n",
    " [0]\n",
    " [1]\n",
    " [0]\n",
    " [0]\n",
    " [0]\n",
    " [0]]\n",
    "\n",
    "The word _ablak_ is not in dictionary!\n",
    "None\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Continuous bag of words (CBOW) concept\n",
    "\n",
    "In the CBOW model we try to fit a model that can predict a word by it's neighbors. We expect that if we manage to fit such a model, the inner representation of the model will capture the meaning of the different words.  \n",
    "\n",
    "The number of neighbors is a parameter, for 4 neighbors it looks like (the **red is the target** word, the blue words are the input, the neighbors):  \n",
    "<font color=\"blue\">The quick</font> <font color=\"red\">brown</font> <font color=\"blue\">fox jumps</font> over the lazy dog  \n",
    "The <font color=\"blue\">quick brown</font> <font color=\"red\">fox</font> <font color=\"blue\">jumps over</font> the lazy dog  \n",
    "The quick <font color=\"blue\">brown fox</font> <font color=\"red\">jumps</font> <font color=\"blue\">over the</font> lazy dog  \n",
    "The quick brown <font color=\"blue\">fox jumps</font> <font color=\"red\">over</font> <font color=\"blue\">the  lazy</font> dog  \n",
    "\n",
    "As it can be seen above from a sentence it is easy to create many training examples!\n",
    "\n",
    "#### Let's make a CBOW example generator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def generate_CBOW_example(start_pos, half_window, tokens):\n",
    "    \"\"\"\n",
    "        Generated CBOW training examples from tokens\n",
    "        Input: \n",
    "            * start_pos:   the position of the first example\n",
    "            * half_window: number of tokens on one-side\n",
    "            * tokens: list of tokens\n",
    "        Output:\n",
    "            * X: input words (the neighbors)\n",
    "            * Y: target word\n",
    "    \"\"\"     \n",
    "    \n",
    "    ###Start code here\n",
    "    \n",
    "    target_pos = start_pos+half_window\n",
    "    X = tokens[start_pos:target_pos] + tokens[(target_pos+1):(target_pos+1) + half_window]\n",
    "    Y = tokens[target_pos]\n",
    "    \n",
    "    ###End code here\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['a', 'corpus', 'contain', 'texts'], 'may')\n",
      "(['may', 'contain', 'in', 'a'], 'texts')\n",
      "(['a', 'corpus', 'may', 'texts', 'in', 'a'], 'contain')\n"
     ]
    }
   ],
   "source": [
    "corpus_tokens = tokenizer(corpus.lower())\n",
    "\n",
    "print(generate_CBOW_example(0, 2, corpus_tokens))\n",
    "print(generate_CBOW_example(2, 2, corpus_tokens))\n",
    "print(generate_CBOW_example(0, 3, corpus_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "<pre>\n",
    "(['a', 'corpus', 'contain', 'texts'], 'may')\n",
    "(['may', 'contain', 'in', 'a'], 'texts')\n",
    "(['a', 'corpus', 'may', 'texts', 'in', 'a'], 'contain')\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Skip-gram concept\n",
    "\n",
    "Skip-gram is the opposite of the CBOW. Now starting from a single word we want to predict it's neighbors.\n",
    "\n",
    "In the skip-gram model we try to fit a model that can predict a word's neighbors. We expect that if we manage to fit such a model, the inner representation of the model will capture the meaning of the different words.  \n",
    "\n",
    "The number of neighbors is a parameter, for 4 neighbors it looks like (the **blue are the target** words, the red word is the input):  \n",
    "<font color=\"blue\">The quick</font> <font color=\"red\">brown</font> <font color=\"blue\">fox jumps</font> over the lazy dog  \n",
    "The <font color=\"blue\">quick brown</font> <font color=\"red\">fox</font> <font color=\"blue\">jumps over</font> the lazy dog  \n",
    "The quick <font color=\"blue\">brown fox</font> <font color=\"red\">jumps</font> <font color=\"blue\">over the</font> lazy dog  \n",
    "The quick brown <font color=\"blue\">fox jumps</font> <font color=\"red\">over</font> <font color=\"blue\">the  lazy</font> dog  \n",
    "\n",
    "As it can be seen above from a sentence it is easy to create many training examples!\n",
    "\n",
    "#### Let's make a skip-gram example generator!\n",
    "#### Luckily it is very easy now. We only need to change X and Y in the CBOW generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skip_gram_example(start_pos, half_window, tokens):\n",
    "    \"\"\"\n",
    "        Generated skip-gram training examples from tokens\n",
    "        Input: \n",
    "            * start_pos:   the position of the first example\n",
    "            * half_window: number of tokens on one-side\n",
    "            * tokens: list of tokens\n",
    "        Output:\n",
    "            * x: input word\n",
    "            * y: target words\n",
    "    \"\"\"     \n",
    "    \n",
    "    X, Y = generate_CBOW_example(start_pos, half_window, tokens)\n",
    "    x = Y\n",
    "    y = X\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Keras models\n",
    "\n",
    "Skip-gram is said to word better for smaller amount of training data and for rare words, but CBOW is faster to train and said to be better for frequent words.\n",
    "\n",
    "If we have a corpus with $N$ tokens then we will have $N$ training examples for CBOW and $\\approx window\\_size \\cdot N$ training examples for skip-gram. \n",
    "\n",
    "Training these models can be really slow so a few other tricks are used for training. For the simplicity we won't implement them now.\n",
    "\n",
    "## 6.1 CBOW model\n",
    "\n",
    "In the model we take a training example which contains $N$ input words and a target word. \n",
    "\n",
    "<img src='images/cbow.png'></img>\n",
    "<center>[Mikolov: Efficient Estimation of Word Representations in\n",
    "Vector Space, 2013]</center>\n",
    "\n",
    "The model is the following:\n",
    " 1. convert the $N$ input words to one-hot encoded ($V$ long) vectors and average them!\n",
    " 2. add a hidden layer of $d$ neurons (no activation, no bias, just the matrix multiplication)\n",
    " 3. add an output layer with $V$ neurons (no bias) and softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steve/Setup/deep/env3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Dense in module keras.layers.core:\n",
      "\n",
      "class Dense(keras.engine.topology.Layer)\n",
      " |  Just your regular densely-connected NN layer.\n",
      " |  \n",
      " |  `Dense` implements the operation:\n",
      " |  `output = activation(dot(input, kernel) + bias)`\n",
      " |  where `activation` is the element-wise activation function\n",
      " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
      " |  created by the layer, and `bias` is a bias vector created by the layer\n",
      " |  (only applicable if `use_bias` is `True`).\n",
      " |  \n",
      " |  Note: if the input to the layer has a rank greater than 2, then\n",
      " |  it is flattened prior to the initial dot product with `kernel`.\n",
      " |  \n",
      " |  # Example\n",
      " |  \n",
      " |  ```python\n",
      " |      # as first layer in a sequential model:\n",
      " |      model = Sequential()\n",
      " |      model.add(Dense(32, input_shape=(16,)))\n",
      " |      # now the model will take as input arrays of shape (*, 16)\n",
      " |      # and output arrays of shape (*, 32)\n",
      " |  \n",
      " |      # after the first layer, you don't need to specify\n",
      " |      # the size of the input anymore:\n",
      " |      model.add(Dense(32))\n",
      " |  ```\n",
      " |  \n",
      " |  # Arguments\n",
      " |      units: Positive integer, dimensionality of the output space.\n",
      " |      activation: Activation function to use\n",
      " |          (see [activations](../activations.md)).\n",
      " |          If you don't specify anything, no activation is applied\n",
      " |          (ie. \"linear\" activation: `a(x) = x`).\n",
      " |      use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |      kernel_initializer: Initializer for the `kernel` weights matrix\n",
      " |          (see [initializers](../initializers.md)).\n",
      " |      bias_initializer: Initializer for the bias vector\n",
      " |          (see [initializers](../initializers.md)).\n",
      " |      kernel_regularizer: Regularizer function applied to\n",
      " |          the `kernel` weights matrix\n",
      " |          (see [regularizer](../regularizers.md)).\n",
      " |      bias_regularizer: Regularizer function applied to the bias vector\n",
      " |          (see [regularizer](../regularizers.md)).\n",
      " |      activity_regularizer: Regularizer function applied to\n",
      " |          the output of the layer (its \"activation\").\n",
      " |          (see [regularizer](../regularizers.md)).\n",
      " |      kernel_constraint: Constraint function applied to\n",
      " |          the `kernel` weights matrix\n",
      " |          (see [constraints](../constraints.md)).\n",
      " |      bias_constraint: Constraint function applied to the bias vector\n",
      " |          (see [constraints](../constraints.md)).\n",
      " |  \n",
      " |  # Input shape\n",
      " |      nD tensor with shape: `(batch_size, ..., input_dim)`.\n",
      " |      The most common situation would be\n",
      " |      a 2D input with shape `(batch_size, input_dim)`.\n",
      " |  \n",
      " |  # Output shape\n",
      " |      nD tensor with shape: `(batch_size, ..., units)`.\n",
      " |      For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
      " |      the output would have shape `(batch_size, units)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dense\n",
      " |      keras.engine.topology.Layer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the layer weights.\n",
      " |      \n",
      " |      Must be implemented on all layers that have weights.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Keras tensor (future input to layer)\n",
      " |              or list/tuple of Keras tensors to reference\n",
      " |              for weight shape computations.\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      Assumes that the layer will be built\n",
      " |      to match that input shape provided.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Container` (one layer of abstraction above).\n",
      " |      \n",
      " |      # Returns\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.topology.Layer:\n",
      " |  \n",
      " |  __call__(self, inputs, **kwargs)\n",
      " |      Wrapper around self.call(), for handling internal references.\n",
      " |      \n",
      " |      If a Keras tensor is passed:\n",
      " |          - We call self._add_inbound_node().\n",
      " |          - If necessary, we `build` the layer to match\n",
      " |              the _keras_shape of the input(s).\n",
      " |          - We update the _keras_shape of every input tensor with\n",
      " |              its new shape (obtained via self.compute_output_shape).\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |          - We update the _keras_history of the output tensor(s)\n",
      " |              with the current layer.\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Can be a tensor or list/tuple of tensors.\n",
      " |          **kwargs: Additional keyword arguments to be passed to `call()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output of the layer's `call` method.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case the layer is missing shape information\n",
      " |              for its `build` call.\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Add losses to the layer.\n",
      " |      \n",
      " |      The loss may potentially be conditional on some inputs tensors,\n",
      " |      for instance activity losses are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          losses: loss tensor or list of loss tensors\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the losses as conditional on these inputs.\n",
      " |              If None is passed, the loss is assumed unconditional\n",
      " |              (e.g. L2 weight regularization, which only depends\n",
      " |              on the layer's weights variables, not on any inputs tensors).\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add updates to the layer.\n",
      " |      \n",
      " |      The updates may potentially be conditional on some inputs tensors,\n",
      " |      for instance batch norm updates are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          updates: update op or list of update ops\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the updates as conditional on these inputs.\n",
      " |              If None is passed, the updates are assumed unconditional.\n",
      " |  \n",
      " |  add_weight(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)\n",
      " |      Adds a weight variable to the layer.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          name: String, the name for the weight variable.\n",
      " |          shape: The shape tuple of the weight.\n",
      " |          dtype: The dtype of the weight.\n",
      " |          initializer: An Initializer instance (callable).\n",
      " |          regularizer: An optional Regularizer instance.\n",
      " |          trainable: A boolean, whether the weight should\n",
      " |              be trained via backprop or not (assuming\n",
      " |              that the layer itself is also trainable).\n",
      " |          constraint: An optional Constraint instance.\n",
      " |      \n",
      " |      # Returns\n",
      " |          The created weight variable.\n",
      " |  \n",
      " |  assert_input_compatibility(self, inputs)\n",
      " |      Checks compatibility between the layer and provided inputs.\n",
      " |      \n",
      " |      This checks that the tensor(s) `input`\n",
      " |      verify the input assumptions of the layer\n",
      " |      (if any). If not, exceptions are raised.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case of mismatch between\n",
      " |              the provided inputs and the expectations of the layer.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      # Returns\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An integer count.\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: if the layer isn't yet built\n",
      " |              (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.engine.topology.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Container), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.topology.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  built\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input shape tuple\n",
      " |          (or list of input shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  losses\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output tensor or list of output tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one inbound node,\n",
      " |      or if all inbound nodes have the same output shape.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output shape tuple\n",
      " |          (or list of input shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  weights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def get_keras_cbow(hidden_dim, dictionary_size):\n",
    "    \"\"\"\n",
    "        Generate keras model for CBOW model\n",
    "        Input: \n",
    "            * hidden_dim: number of nurons in the hidden layer\n",
    "            * dictionary_size: length of the dictionary (output/input size)\n",
    "        Output:\n",
    "            * model: keras model that implements CBOW \n",
    "    \"\"\"\n",
    "    cbow = Sequential()\n",
    "    ###Start code here\n",
    "    \n",
    "    # add a Dense layer with hidden_dim neurons. Input_dim is the dictionary_size \n",
    "    # we don't need nor activation neither bias!\n",
    "    cbow.add(Dense(hidden_dim, input_shape=(dictionary_size,), use_bias=False, activation=\"linear\"))\n",
    "    \n",
    "    # add a Dense layer with dictionary_size neurons. We don't need bias. Activation is softmax.\n",
    "    cbow.add(Dense(dictionary_size, use_bias=False, activation=\"softmax\"))\n",
    "    \n",
    "    ###End code here\n",
    "    \n",
    "    return cbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's train it on out toy corpus. We should see the loss decreasing, but we do not expect anything serious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = get_dictionary(corpus.lower())\n",
    "cbow_model = get_keras_cbow(30, len(dic))\n",
    "cbow_model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 , Loss: 4.762315397420205\n",
      "Epoch 02 , Loss: 4.582635418442655\n",
      "Epoch 03 , Loss: 4.348783166940547\n",
      "Epoch 04 , Loss: 4.065531326719552\n",
      "Epoch 05 , Loss: 3.7850974522346306\n",
      "Epoch 06 , Loss: 3.5245633125305176\n",
      "Epoch 07 , Loss: 3.266812979682418\n",
      "Epoch 08 , Loss: 3.004162034712547\n",
      "Epoch 09 , Loss: 2.740534950632694\n",
      "Epoch 10 , Loss: 2.4827156182655616\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "half_window = 2\n",
    "corpus_tokens = tokenizer(corpus.lower())\n",
    "max_pos_start = len(corpus_tokens)-2*half_window\n",
    "\n",
    "for iteration in range(epochs): #iterate the corpus epochs times\n",
    "    loss = 0.\n",
    "    \n",
    "    for pos_start in range(max_pos_start): # iterate on the token positions\n",
    "        x, y = generate_CBOW_example(pos_start, half_window, corpus_tokens) # generate training examples\n",
    "        \n",
    "        x = np.array([one_hot_encoded(i, dic) for i in x]).sum(0).T \n",
    "        y = one_hot_encoded(y, dic).T\n",
    "        \n",
    "        loss += cbow_model.train_on_batch(x, y) # train the model. only 1 sample/batch now...\n",
    "\n",
    "    print('Epoch', str(iteration+1).zfill(2), ', Loss:',  loss/(max_pos_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the loss decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 skip-gram model\n",
    "\n",
    "In the model we take a training example which contains an input word and the $N$ target words. We will handle the target words separately, so actually it means $N$ different training X-Y pairs. \n",
    "\n",
    "\n",
    "<img src='images/skip_gram.png'></img>\n",
    "<center>[Mikolov: Efficient Estimation of Word Representations in\n",
    "Vector Space, 2013]</center>\n",
    "\n",
    "The model is the following:\n",
    " 1. convert the input word and one of the target word to one-hot encoded ($V$ long) vector!\n",
    " 2. add a hidden layer of $d$ neurons (no activation, no bias, just the matrix multiplication)\n",
    " 3. add an output layer with $V$ neurons (no bias) and softmax activation.\n",
    " \n",
    "Luckily it is exactly the same model than the CBOW. The only difference is how we create the input and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = get_dictionary(corpus.lower())\n",
    "skip_gram_model = get_keras_cbow(30, len(dic))\n",
    "skip_gram_model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 , Loss: 4.764283511264265\n",
      "Epoch 02 , Loss: 4.638532874012782\n",
      "Epoch 03 , Loss: 4.441418981749164\n",
      "Epoch 04 , Loss: 4.235765141273333\n",
      "Epoch 05 , Loss: 4.0583667304397615\n",
      "Epoch 06 , Loss: 3.892259396551069\n",
      "Epoch 07 , Loss: 3.7324367303493595\n",
      "Epoch 08 , Loss: 3.5810870426006556\n",
      "Epoch 09 , Loss: 3.4414203304889774\n",
      "Epoch 10 , Loss: 3.3155753123612444\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "half_window = 2\n",
    "corpus_tokens = tokenizer(corpus.lower())\n",
    "max_pos_start = len(corpus_tokens)-2*half_window\n",
    "\n",
    "for iteration in range(epochs): #iterate the corpus epochs times\n",
    "    loss = 0.\n",
    "    \n",
    "    for pos_start in range(max_pos_start): # iterate on the token positions\n",
    "        x, y = generate_skip_gram_example(pos_start, half_window, corpus_tokens) # generate training examples\n",
    "        \n",
    "        x = one_hot_encoded(x, dic).T  \n",
    "        y = np.array([one_hot_encoded(i, dic) for i in y])\n",
    "        \n",
    "        for i in y: # iterate on the target words\n",
    "            loss += skip_gram_model.train_on_batch(x, i.T) # train the model. only 1 sample/batch now...\n",
    "\n",
    "    print('Epoch', str(iteration+1).zfill(2), ', Loss:',  loss/(max_pos_start*len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the loss decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the end of the homework. The deadline is 2018.04.24.\n",
    "\n",
    "In the next homework we will continue to our smiley generator!"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
